{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18a5b234",
   "metadata": {},
   "source": [
    "### Human Shade Detection\n",
    "\n",
    "This code can:\n",
    "- Person Tracking: Capture coordinate information for each person in every frame\n",
    "- Shade Detection: Identify shadow regions within the image\n",
    "- Person-Shade Relationship Analysis: Determine whether individuals are positioned within shadowed areas\n",
    "\n",
    "Input: preprocessed video\n",
    "\n",
    "Output: csv and annotated video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27d65368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "from inference_sdk import InferenceHTTPClient\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from ultralytics import YOLO, solutions\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "\n",
    "from roboflow import Roboflow\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1805a8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check and use gpu to speed up inference\n",
    "\n",
    "import torch\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "# else:\n",
    "#     print(\"Using CPU\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5b8b88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Workfolder\\Computer-vision\\shade-human\\data\\raw-video\\Kabukicho\\Kabukicho_240825_truncated.mp4\n",
      "Kabukicho_240825_truncated\n"
     ]
    }
   ],
   "source": [
    "video_path = r'E:\\Workfolder\\Computer-vision\\shade-human\\data\\raw-video\\Kabukicho\\Kabukicho_240825_truncated.mp4' # replace this with the path to your video file\n",
    "file_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "\n",
    "print(video_path)\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddfbd21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of frames in the video：631\n",
      "Video frame rate：59.999998098256796\n"
     ]
    }
   ],
   "source": [
    "# check number of frames in the video to estimate inference time\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get the file name\n",
    "file_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "\n",
    "# Get the total number of frames of the video\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "print(f\"The total number of frames in the video：{total_frames}\\nVideo frame rate：{original_fps}\")\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e1a4ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 416x640 11 persons, 1020.5ms\n",
      "Speed: 3.7ms preprocess, 1020.5ms inference, 0.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 11 persons, 1027.6ms\n",
      "Speed: 6.0ms preprocess, 1027.6ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 11 persons, 989.0ms\n",
      "Speed: 4.6ms preprocess, 989.0ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 10 persons, 818.0ms\n",
      "Speed: 2.2ms preprocess, 818.0ms inference, 0.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 11 persons, 975.6ms\n",
      "Speed: 4.1ms preprocess, 975.6ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 11 persons, 902.8ms\n",
      "Speed: 4.9ms preprocess, 902.8ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Processing complete. Results saved in output/human-and-shade-tracking_Kabukicho_240825_truncated.avi and output/detections_Kabukicho_240825_truncated.csv.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "function: detect people whether in the shadow from the video\n",
    "change the parameters: skip\n",
    "'''\n",
    "skip = 2  # Set the number of frame skips\n",
    "\n",
    "output_dir = \"output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "output_video_path = \"output/human-and-shade-tracking_{}.avi\".format(file_name)\n",
    "csv_file = 'output/detections_{}.csv'.format(file_name)\n",
    "\n",
    "# Initialize Roboflow client for shadow detection\n",
    "CLIENT = InferenceHTTPClient(api_url=\"https://detect.roboflow.com\", api_key=\"xxx")\n",
    "\n",
    "# Load the YOLO model for person detection\n",
    "model_person = YOLO('yolo11l.pt').to(device)\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))\n",
    "\n",
    "# Initialize CSV file\n",
    "csv_headers = ['frame', 'person_id', 'bottom_mid_x', 'bottom_mid_y', 'in_shadow']\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(csv_headers)\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "# Statistics variables\n",
    "total_people = 0\n",
    "people_in_shadow = 0\n",
    "people_outside_shadow = 0\n",
    "\n",
    "# Create a window and set its size\n",
    "# cv2.namedWindow(\"instance-segmentation-object-tracking\", cv2.WINDOW_NORMAL)\n",
    "# cv2.resizeWindow(\"instance-segmentation-object-tracking\", 800, 600)\n",
    "\n",
    "# Start processing each frame\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Perform people count detection every frame, change here\n",
    "    if frame_count % skip == 0:\n",
    "        # Get the shadow boundary of the current frame\n",
    "        shadow_result = CLIENT.infer(frame, model_id=\"shade-detection/1\")\n",
    "        boundaries = []\n",
    "\n",
    "        # Data structure for single image inference\n",
    "        if 'predictions' in shadow_result: \n",
    "            predictions = shadow_result['predictions']\n",
    "            for prediction in predictions:\n",
    "                points = prediction.get(\"points\", [])\n",
    "                if points:\n",
    "                    boundary = [(int(point['x']), int(point['y'])) for point in points]\n",
    "                    boundaries.append(boundary)  # Add the point coordinates of the shadow area to the boundaries list\n",
    "\n",
    "        annotator = Annotator(frame, line_width=2)\n",
    "        \n",
    "        # Reset statistics variables\n",
    "        people_in_shadow = 0\n",
    "        people_outside_shadow = 0\n",
    "        total_people = 0\n",
    "        detections = [] \n",
    "\n",
    "        # Draw the shadow polygon\n",
    "        for boundary in boundaries:\n",
    "            boundary_points = np.array(boundary, np.int32).reshape((-1, 1, 2))\n",
    "            cv2.polylines(frame, [boundary_points], isClosed=True, color=(255, 0, 0), thickness=2)\n",
    "\n",
    "        # people detection\n",
    "        # person_results = model_person(frame, classes=[0], conf=0.05, show=False)  \n",
    "        person_results = model_person.track(frame, persist=True, classes=[0], conf=0.05, show=False) # 0 is the category ID of the person, close the automatic display window\n",
    "        \n",
    "        if person_results and len(person_results) > 0:\n",
    "            boxes = person_results[0].boxes\n",
    "\n",
    "            if boxes and len(boxes) > 0:\n",
    "                # Extract bounding boxes\n",
    "                xyxy = boxes.xyxy.cpu().numpy()  # Get the box in xyxy format\n",
    "                track_ids = np.arange(len(xyxy))  # Generate a unique ID for each box\n",
    "\n",
    "                for i, (x1, y1, x2, y2) in enumerate(xyxy):\n",
    "                    bottom_mid_x = (x1 + x2) / 2  # X coordinate of the midpoint of the bottom edge\n",
    "                    bottom_mid_y = y2  # Y coordinate of the bottom edge\n",
    "                    in_shadow = False\n",
    "\n",
    "                    # Check if in shadow\n",
    "                    for boundary in boundaries:\n",
    "                        if cv2.pointPolygonTest(np.array(boundary, np.int32), (int(bottom_mid_x), int(bottom_mid_y)), False) >= 0:\n",
    "                            in_shadow = True\n",
    "                            people_in_shadow += 1\n",
    "                            break\n",
    "                    else:\n",
    "                        people_outside_shadow += 1\n",
    "\n",
    "                    # Record test results\n",
    "                    detections.append([frame_count, track_ids[i], bottom_mid_x, bottom_mid_y, in_shadow])\n",
    "                    total_people += 1\n",
    "\n",
    "                    # Display the detection box, text and the dot at the bottom midpoint\n",
    "                    color = colors(int(track_ids[i]), True)\n",
    "                    txt_color = annotator.get_txt_color(color)\n",
    "                    label = f\"ID {track_ids[i]} {'in shadow' if in_shadow else 'out shadow'}\"\n",
    "                    annotator.box_label([x1, y1, x2, y2], label=label, color=color, txt_color=txt_color)\n",
    "                    cv2.circle(frame, (int(bottom_mid_x), int(bottom_mid_y)), 5, (0, 0, 255), -1)  \n",
    "\n",
    "        # Write the results to CSV\n",
    "        with open(csv_file, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(detections)\n",
    "\n",
    "        # Display the statistics results on the frame\n",
    "        text = f\"Frame {frame_count}: Total: {total_people}, In Shadow: {people_in_shadow}, Outside Shadow: {people_outside_shadow}\"\n",
    "        cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        out.write(frame)\n",
    "        cv2.imshow(\"instance-segmentation-object-tracking\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Processing complete. Results saved in {output_video_path} and {csv_file}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
